# -*- coding: utf-8 -*-
"""U1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UNB8-ptuTNEFy9DHkhOkvvLSz_4gYpGZ
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
from scipy.stats import zscore
from sklearn import tree
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay, roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn import preprocessing
from sklearn.decomposition import PCA
import scipy.stats
from sklearn.metrics import PrecisionRecallDisplay
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.neighbors import KNeighborsClassifier


pd.set_option('display.float_format', lambda x: '%.2f' % x)
# %matplotlib inline

data = pd.read_csv('diabetes_prediction_dataset.csv')

data.head()

data.describe()

print(data.columns)

#Find number of rows and columns

num_rows, num_columns = data.shape

print("Number of rows:", num_rows)
print("Number of columns:", num_columns)

#Distribution
data['diabetes'].value_counts()

plt.pie(data['diabetes'].value_counts(), labels = ['non-diabetic', 'diabetic'],
       autopct = '%1.1f%%')
plt.title("Distribution of diabetics in dataset")
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(x='diabetes', y='HbA1c_level', data=data)
plt.title('HbA1c_level by Diabetes Status')
plt.xlabel('Diabetes Status')
plt.ylabel('HbA1c_level')
plt.xticks([0, 1], ['Non-Diabetic', 'Diabetic'])  # Adjust based on your 'diabetes' column encoding
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(x='diabetes', y='blood_glucose_level', data=data)
plt.title('Blood Glucose Levels by Diabetes Status')
plt.xlabel('Diabetes Status')
plt.ylabel('Blood Glucose Level')
plt.xticks([0, 1], ['Non-Diabetic', 'Diabetic'])  # Adjust based on your 'diabetes' column encoding
plt.show()

## seperating numerical and categorical features for analysis

cat_data = data.select_dtypes(include='object')
num_data = data.select_dtypes(exclude='object')
print("Categorical Features: ", cat_data.columns.to_list())
print("Numerical Features: ", num_data.columns.to_list())

#Find Missing Values

missing_values = data.isnull().sum().sum()
percentage_missing = (missing_values / data.shape[0]) * 100
print("Percentage of missing values:", percentage_missing)

#Convert Categorical variables into binary and numeric
label_encoder = preprocessing.LabelEncoder()
data['smoking_history'] = label_encoder.fit_transform(data['smoking_history'])
data['gender'] = label_encoder.fit_transform(data['gender'])
data.head()

numeric_data = data.select_dtypes(include='number')
print(numeric_data)

data.fillna(data.median(), inplace=True)

print(data.isnull().sum())

# Selecting the numerical columns (excluding binary columns for hypertension, heart_disease, diabetes)
numerical_columns = ['age', 'bmi', 'HbA1c_level','smoking_history', 'blood_glucose_level']

# Calculate Z-scores for numeric columns
z_scores = data[numerical_columns].apply(zscore)

print(z_scores)

# Define threshold for outlier detection (e.g., Z-Score > 3)
threshold = 3
# Find outliers
outliers = data[z_scores > threshold]
print(outliers)

# Count outliers
outliers_count = (z_scores.abs() > threshold).sum().sum()

print("Number of outliers:", outliers_count)

# normalization


# Initialize the MinMaxScaler
scaler = MinMaxScaler()

# Fit the scaler to the data and transform it
data[numerical_columns] = scaler.fit_transform(data[numerical_columns])

# Display the first few rows to verify the normalization
print(data.head())

data.head()

plt.figure(figsize=(10,8))
sns.heatmap(data=num_data.corr(),annot=True)
plt.show()

# Calculate the correlation matrix
corr_matrix = data.corr()

# Plot the heatmap
plt.figure(figsize=(10, 8))  # Set the figure size as desired
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm', cbar=True, square=True)
plt.title('Correlation Matrix Heatmap')
plt.show()

X = data.drop(columns=['diabetes'], axis=1)
y = data['diabetes']

X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)

X_train = pd.DataFrame((X_train_raw), columns=X_train_raw.columns)
X_test =  pd.DataFrame((X_test_raw), columns=X_test_raw.columns)
X_test.head()

sns.pairplot(num_data)
plt.show()

num_data.hist(bins=10, figsize=(10, 8))
plt.show()

sns.set(rc={'figure.figsize':(11.7,8.27)})
sns.boxplot(data=num_data)
plt.title('Boxplots of Numerical Features')
plt.show()

# # Create a bar chart
for feature in cat_data.columns.to_list():
    uniques, counts = np.unique(cat_data[feature], return_counts=True)
    plt.bar(uniques, counts)
    plt.xlabel(feature)
    plt.ylabel('Values Count')
    plt.title(f'Bar Chart of {feature}')
    plt.show()

feature = 'diabetes'
uniques, counts = np.unique(num_data[feature], return_counts=True)
plt.bar(uniques, counts)
plt.xlabel(feature)
plt.xlim()
plt.ylabel('Values Count')
plt.title(f'Bar Chart of {feature}')
plt.show()

from sklearn.preprocessing import MinMaxScaler
num_features = num_data.columns.to_list()[:-1]
scaler = MinMaxScaler()
data[num_features] = scaler.fit_transform(data[num_features])

from sklearn.preprocessing import LabelEncoder
cat_features = cat_data.columns.to_list()
encoder = LabelEncoder()
for feature in cat_features:
    data[feature] = encoder.fit_transform(data[feature])

print(data.head(5))

sns.set(rc={'figure.figsize':(11.7,8.27)})
sns.boxplot(data=data[num_features])
plt.title('Boxplots of Numerical Features')
plt.show()

sns.set(rc={'figure.figsize':(15.7,8.27)})
sns.boxplot(data=data)
plt.title('Boxplots of Numerical Features')
plt.show()

from sklearn.model_selection import train_test_split
features_list = data.columns.to_list()
features_data, target_data = data[features_list[:-1]], data[features_list[-1]]
print("Features: ", features_data.columns)
print("Target: ", target_data.name)
X_train, X_test, y_train, y_test = train_test_split(features_data, target_data, stratify=target_data, test_size=0.25)

# train a logistic regression model on the training set
from sklearn.linear_model import LogisticRegression


# instantiate the model
logreg = LogisticRegression(solver='liblinear', random_state=0)


# fit the model
logreg.fit(X_train, y_train)

y_pred_test = logreg.predict(X_test)

y_pred_test

# probability of getting output as 0 - no diabetes

logreg.predict_proba(X_test)[:, 0]

# probability of getting output as 0 - diabetes
logreg.predict_proba(X_test)[:, 1]

from sklearn.metrics import accuracy_score

print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred_test)))

y_pred_train = logreg.predict(X_train)

y_pred_train

print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))

# print the scores on training and test set

print('Training set score: {:.4f}'.format(logreg.score(X_train, y_train)))

print('Test set score: {:.4f}'.format(logreg.score(X_test, y_test)))

# fit the Logsitic Regression model with C=001

# instantiate the model
logreg001 = LogisticRegression(C=0.01, solver='liblinear', random_state=0)


# fit the model
logreg001.fit(X_train, y_train)

# print the scores on training and test set

print('Training set score: {:.4f}'.format(logreg001.score(X_train, y_train)))

print('Test set score: {:.4f}'.format(logreg001.score(X_test, y_test)))

# Print the Confusion Matrix and slice it into four pieces

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_pred_test)

print('Confusion matrix\n\n', cm)

print('\nTrue Positives(TP) = ', cm[0,0])

print('\nTrue Negatives(TN) = ', cm[1,1])

print('\nFalse Positives(FP) = ', cm[0,1])

print('\nFalse Negatives(FN) = ', cm[1,0])

# visualize confusion matrix with seaborn heatmap

cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'],
                                 index=['Predict Positive:1', 'Predict Negative:0'])

sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')

from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_test))

TP = cm[0,0]
TN = cm[1,1]
FP = cm[0,1]
FN = cm[1,0]

# print classification accuracy

classification_accuracy = (TP + TN) / float(TP + TN + FP + FN)

print('Classification accuracy : {0:0.4f}'.format(classification_accuracy))

# print classification error

classification_error = (FP + FN) / float(TP + TN + FP + FN)

print('Classification error : {0:0.4f}'.format(classification_error))

# print precision score

precision = TP / float(TP + FP)


print('Precision : {0:0.4f}'.format(precision))

recall = TP / float(TP + FN)

print('Recall or Sensitivity : {0:0.4f}'.format(recall))

true_positive_rate = TP / float(TP + FN)


print('True Positive Rate : {0:0.4f}'.format(true_positive_rate))

false_positive_rate = FP / float(FP + TN)


print('False Positive Rate : {0:0.4f}'.format(false_positive_rate))

specificity = TN / (TN + FP)

print('Specificity : {0:0.4f}'.format(specificity))

# print the first 10 predicted probabilities of two classes- 0 and 1

y_pred_prob = logreg.predict_proba(X_test)[0:10]

y_pred_prob

from sklearn.preprocessing import binarize

for i in range(1,10):

    cm1=0

    y_pred1 = logreg.predict_proba(X_test)[:,1]

    y_pred1 = y_pred1.reshape(-1,1)

    y_pred2 = binarize(y_pred1, threshold = i/10)

    y_pred2 = np.where(y_pred2 == 1, 1, 0)

    cm1 = confusion_matrix(y_test, y_pred2)
    print(cm1)

    print ('With',i/10,'threshold the Confusion Matrix ','\n\n',cm1,'\n')

# plot ROC Curve

from sklearn.metrics import roc_curve

fpr, tpr, thresholds = roc_curve(y_test, y_pred_test, pos_label = 1)

plt.figure(figsize=(6,4))

plt.plot(fpr, tpr, linewidth=2)

plt.plot([0,1], [0,1], 'k--' )

plt.rcParams['font.size'] = 12

plt.title('ROC curve for classifier')

plt.xlabel('False Positive Rate (1 - Specificity)')

plt.ylabel('True Positive Rate (Sensitivity)')

plt.show()

# compute ROC AUC

from sklearn.metrics import roc_auc_score

ROC_AUC = roc_auc_score(y_test, y_pred_test)

print('ROC AUC : {:.4f}'.format(ROC_AUC))

# calculate cross-validated ROC AUC

from sklearn.model_selection import cross_val_score

Cross_validated_ROC_AUC = cross_val_score(logreg, X_train, y_train, cv=5, scoring='roc_auc').mean()

print('Cross validated ROC AUC : {:.4f}'.format(Cross_validated_ROC_AUC))

# Applying 5-Fold Cross Validation

from sklearn.model_selection import cross_val_score

scores = cross_val_score(logreg, X_train, y_train, cv = 5, scoring='accuracy')

print('Cross-validation scores:{}'.format(scores))

# compute Average cross-validation score

print('Average cross-validation score: {:.4f}'.format(scores.mean()))

from sklearn.model_selection import GridSearchCV


parameters = [{'penalty':['l1','l2']},
              {'C':[1, 10, 100, 1000]}]



grid_search = GridSearchCV(estimator = logreg,
                           param_grid = parameters,
                           scoring = 'accuracy',
                           cv = 5,
                           verbose=0)


grid_search.fit(X_train, y_train)

# examine the best model

# best score achieved during the GridSearchCV
print('GridSearch CV best score : {:.4f}\n\n'.format(grid_search.best_score_))

# print parameters that give the best results
print('Parameters that give the best results :','\n\n', (grid_search.best_params_))

# print estimator that was chosen by the GridSearch
print('\n\nEstimator that was chosen by the search :','\n\n', (grid_search.best_estimator_))

# calculate GridSearch CV score on test set

print('GridSearch CV score on test set: {0:0.4f}'.format(grid_search.score(X_test, y_test)))

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix

# Create a KNN Classifier
knn = KNeighborsClassifier(n_neighbors=3)

# Train the model using the training sets
knn.fit(X_train, y_train)

# Predict the response for test dataset
y_pred = knn.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix
plt.figure(figsize=(10, 10))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.show()

# Print accuracy and F1 score
print('Accuracy:', accuracy)
print('F1 Score:', f1)

training_accuracy = []
test_accuracy = []

# try n_neighbors from 1 to 10
possible_neighbors = range(1, 11)

for n_neighbors in possible_neighbors:
    clf = KNeighborsClassifier(n_neighbors=n_neighbors)
    clf.fit(X_train, y_train)

    # training accuracy
    training_accuracy.append(clf.score(X_train, y_train))

    # testing accuracy
    test_accuracy.append(clf.score(X_test, y_test))

plt.plot(possible_neighbors, training_accuracy, label="training accuracy")
plt.plot(possible_neighbors, test_accuracy, label="test accuracy");
plt.ylim((0,1))
plt.ylabel("Accuracy");
plt.xlabel("n_neighbors");
plt.legend();

model = KNeighborsClassifier(n_neighbors=6)
model.fit(X_train, y_train)

y_pred_test2= model.predict(X_test)
y_pred_train = model.predict(X_train)

print(classification_report(y_train, y_pred_train))

print(classification_report(y_test, y_pred_test))

cm = confusion_matrix(y_test, y_pred_test, labels=model.classes_);
ConfusionMatrixDisplay.from_predictions(y_pred=y_pred_test, y_true=y_test, display_labels=model.classes_, cmap='Blues');

RocCurveDisplay.from_estimator(model, X_test, y_test)
plt.show()

roc_auc_score(y_test, y_pred_test)

display = PrecisionRecallDisplay.from_estimator(model, X_test, y_test, name="Decision Tree")
_ = display.ax_.set_title("2-class Precision-Recall curve")

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
from sklearn.tree import plot_tree

# Create a Decision Tree Classifier
clf = DecisionTreeClassifier(max_depth=6, random_state=42)

# Train the model using the training sets
clf.fit(X_train, y_train)

# Predict the response for the test dataset
y_pred = clf.predict(X_test)

plot_tree(clf, filled=True)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix
plt.figure(figsize=(10, 10))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted label')
plt.ylabel('True label')

plt.show()

# Print accuracy and F1 score
print('Accuracy:', accuracy)
print('F1 Score:', f1)

model = tree.DecisionTreeClassifier()

model.fit(X_train, y_train)

fig = plt.figure(figsize=(20,15))
tree.plot_tree(model);

feature_importances = pd.Series(model.feature_importances_, model.feature_names_in_).sort_values()
feature_importances.plot.barh();
plt.title('Decision Tree Feature Importance');

y_pred_test = model.predict(X_test)
y_pred_train= model.predict(X_train)

train_decision_tree_report = classification_report(y_train, y_pred_train)
print(train_decision_tree_report)

test_decision_tree_report = classification_report(y_test, y_pred_test)
print(test_decision_tree_report)

RocCurveDisplay.from_estimator(model, X_test, y_test)
plt.show()

roc_auc_score(y_test, y_pred_test)

cm = confusion_matrix(y_test, y_pred_test, labels=model.classes_);
ConfusionMatrixDisplay.from_predictions(y_pred=y_pred_test, y_true=y_test, display_labels=model.classes_, cmap='Blues');

display = PrecisionRecallDisplay.from_estimator(model, X_test, y_test, name="Decision Tree")
_ = display.ax_.set_title("2-class Precision-Recall curve")